{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_to_df (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ODE_Fit version that uses the IPNewton() optimization algorithm from Optim.jl\n",
    "using Pkg, Plots, XLSX, Dates, DataFrames, Random, Statistics\n",
    "using NBInclude\n",
    "using DifferentialEquations\n",
    "using ForwardDiff, Optim, Sundials\n",
    "@nbinclude(\"ODE_models.ipynb\")\n",
    "@nbinclude(\"Norm_functions.ipynb\")\n",
    "@nbinclude(\"HelperFunctions.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SecondOrderFit (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function SecondOrderFit(;ODE_model, ODE_vars, data_obs, t_obs, t_all, x0, x_LBs, x_UBs, UBs,\n",
    "                         param_opt_indices, param_fix_indices, IC_opt_indices, IC_fix_indices,\n",
    "                         params_fix, ICs_fix, f_calc_ICs, N0, norm, norm_weights, norm_scale,\n",
    "                         optimizer, make_plots::Bool, integrator_options::Dict,\n",
    "                         optim_options, constraints, autodiff, convergence_report::Bool = false)\n",
    "    \n",
    "    #THE ARGUMENTS\n",
    "    #ODE_model: The function that computes the vector dY/dt given a vector Y, the time t, and parameter values. \n",
    "    #ODE_vars: The ODE variable names, given as an array of Strings\n",
    "    #data_obs: A DataFrame of observed data values, where each column corresponds to an ODE variable.\n",
    "    #t_obs: An array of the time values at which the data in data_obs were observed.  \n",
    "    #t_all: An array of time values for which to plot the predicted value \n",
    "    #x0: The initial guess (Array of Reals) for the optimizer     \n",
    "    #x_LBs: The lower bounds for all of the variables being fitted ([param_LBs; IC_LBs])\n",
    "    #x_UBs: The upper bounds for all the variables being fitted (should be [1,1,1,...,l])\n",
    "    #UBs: The original (unscaled) upper bounds on all the parameters & ICs being fitted \n",
    "    \n",
    "    #optimizer: one of IPNewton(), ConjugateGradient(), or GradientDescent()\n",
    "    #optim_options: An object of the form Optim.Options(xtol = xtol_abs, gtol = gtol_rel,...)\n",
    "    #constraints: A TwiceDifferentiableConstraints object \n",
    "    #autodiff: either :finite or :forward \n",
    "    #convergence_report: Whether or not to display the convergence report after the optimization finishes.\n",
    "    \n",
    "    ########################################################################\n",
    "    #Step 1: Get some info needed in order to define the objective function\n",
    "    ########################################################################\n",
    "    \n",
    "    #Scale the observed data \n",
    "    data_obs_scaled = data_obs ./ N0\n",
    "    \n",
    "    vars_to_fit = names(data_obs)    \n",
    "    num_params_opt = length(param_opt_indices)\n",
    "    num_ICs_opt = length(IC_opt_indices)\n",
    "    \n",
    "    num_params = num_params_opt + length(param_fix_indices)\n",
    "    num_ICs_and_IC_ratios = num_ICs_opt + length(IC_fix_indices)\n",
    "    \n",
    "    #Unpack integrator options\n",
    "    rtol = get(integrator_options, :rtol, 1e-8)\n",
    "    atol = get(integrator_options, :atol, 1e-8)\n",
    "    integrator = get(integrator_options, :integrator, Tsit5())\n",
    "    \n",
    "    #Get the original upper bounds \n",
    "    param_UBs = UBs[1:num_params_opt]\n",
    "    IC_UBs = UBs[num_params_opt+1:end]\n",
    "    \n",
    "    #########################################\n",
    "    #Step 2: Define the objective function\n",
    "    #########################################\n",
    "    \n",
    "    function objective(x)\n",
    "        \n",
    "        #Unpack scaled params & ICs/IC ratios\n",
    "        params_opt_scaled = x[1:num_params_opt]\n",
    "        IC_ratios_opt_scaled = x[num_params_opt+1:end]\n",
    "        \n",
    "        #Scale the params & IC ratios back\n",
    "        params_opt = params_opt_scaled .* param_UBs\n",
    "        IC_ratios_opt = IC_ratios_opt_scaled .* IC_UBs\n",
    "        \n",
    "        #Create an array to store the ODE parameters\n",
    "        ODE_params = Array{Real,1}(undef, num_params) \n",
    "        \n",
    "        #Populate ODE_params with the parameter values\n",
    "        ODE_params[param_opt_indices] = params_opt\n",
    "        ODE_params[param_fix_indices] = params_fix\n",
    "        \n",
    "        #Create an array to store the ICs and IC ratios\n",
    "        ICs_and_IC_ratios = Array{Real,1}(undef, num_ICs_and_IC_ratios) \n",
    "        \n",
    "        #Populate ICs_and_IC_ratios \n",
    "        ICs_and_IC_ratios[IC_opt_indices] = IC_ratios_opt\n",
    "        ICs_and_IC_ratios[IC_fix_indices] = ICs_fix\n",
    "        \n",
    "        #Calculate the ODE Initial Conditions (for the integrator)\n",
    "        ODE_ICs = f_calc_ICs(ICs_and_IC_ratios)\n",
    "        \n",
    "        #Scale the initial condition\n",
    "        ODE_ICs_scaled = ODE_ICs ./ N0\n",
    "        \n",
    "        #Get the time interval for which to solve the ODE\n",
    "        t_span = 1.0 .* [t_obs[1], t_obs[end]]\n",
    "        \n",
    "        #Compute the numerical solution of the ODE  \n",
    "        ODE_prob = ODEProblem(ODE_model, ODE_ICs_scaled, t_span, ODE_params)\n",
    "        ODE_sol = solve(ODE_prob, integrator, reltol = rtol, abstol = atol, saveat = t_obs, dt = 0.01)\n",
    "        \n",
    "        #Note: For adaptive step size integrators, the dt argument is just the initial step size.\n",
    "        #Setting an initial dt seems to help the \"Warning initial dt set to NaN\" issue)\n",
    "        \n",
    "        code = ODE_sol.retcode \n",
    "        \n",
    "        #If the integration was successful, compute the norm (aka loss). \n",
    "        if code == :Success\n",
    "        \n",
    "            #Convert ODE_sol to a DataFrame (for convenience)\n",
    "            ODE_sol = DataFrame(ODE_sol', ODE_vars)\n",
    "\n",
    "            norm_sum = 0.0\n",
    "            for var in vars_to_fit\n",
    "                var_pred = ODE_sol[:,var]\n",
    "                var_obs = data_obs_scaled[:,var]\n",
    "                norm_sum += norm(var_pred, var_obs; w = norm_weights)\n",
    "            end\n",
    "            \n",
    "            mean_norm = norm_sum / length(vars_to_fit)\n",
    "            return mean_norm \n",
    "           \n",
    "       #If the integration somehow failed (e.g. due to blow-up), return Inf\n",
    "       else\n",
    "            println(\"Uh oh, the ODE solver failed. The return code is $code\")\n",
    "            println(\"The x values that were passed to objective(): \", x)\n",
    "            return Inf\n",
    "       end\n",
    "        \n",
    "    end\n",
    "    #End of Objective Function definition\n",
    "    \n",
    "    \n",
    "    ###############################\n",
    "    #STEP 3: Call to the Optimizer\n",
    "    ###############################\n",
    "    \n",
    "    local out::Any       #Gets assigned the Optim result object\n",
    "    local AD_fail::Bool  #To keep track of any autodifferentiation fails\n",
    "    \n",
    "  \n",
    "    Random.seed!(2021)\n",
    "    #Attempt to optimize using automatic differentiation (via ForwardDiff).\n",
    "    try \n",
    "        Twice_diff = TwiceDifferentiable(objective, x0, autodiff = :forward)\n",
    "        out = Optim.optimize(Twice_diff, constraints, x0, IPNewton(), optim_options)\n",
    "        AD_fail = false\n",
    "    \n",
    "    #If the optimization w/AD fails, attempt it again using finite differencing instead.  \n",
    "    catch    \n",
    "        println(\"Optimization with ForwardDiff failed. Switching to finite differencing...\", \"\\n\")\n",
    "        Twice_diff = TwiceDifferentiable(objective, x0, autodiff = :finite)\n",
    "        out = Optim.optimize(Twice_diff, constraints, x0, IPNewton(), optim_options)\n",
    "        AD_fail = true\n",
    "    end    \n",
    "        \n",
    "    #The minimizer obtained by the optimizer. This will be an array of Float64's\n",
    "    x_opt = out.minimizer \n",
    "    \n",
    "    #Displays the convergence report when show_trace is true\n",
    "    if convergence_report == true\n",
    "        println(out)\n",
    "    end\n",
    "\n",
    "    \n",
    "    #######################################################################\n",
    "    #STEP 4: Compute the ODE solution using the minimizer that Optim found \n",
    "    #        and plot it against the observed data (if make_plots == true)\n",
    "    #######################################################################\n",
    " \n",
    "    if make_plots == true\n",
    "        y = ODE_sol(ODE_model, ODE_vars, x_opt, params_fix, ICs_fix, param_opt_indices, param_fix_indices,\n",
    "                     IC_opt_indices, IC_fix_indices, param_UBs, IC_UBs, f_calc_ICs, N0, t_all, int_options)\n",
    "        \n",
    "        for var in vars_to_fit\n",
    "           plot(y.t, y[:,var],\n",
    "                 size = (450,350),\n",
    "                 line = :line,\n",
    "                 lw = 1.5,\n",
    "                 label = \"$var (pred)\",\n",
    "                 legend = :topleft,\n",
    "                 xlabel = \"t\",\n",
    "                 ylabel = \"Number\",\n",
    "                 title = \"$var\")\n",
    "            \n",
    "           display(plot!(t_obs, data_obs[:,var],\n",
    "                 line = :scatter,\n",
    "                 markersize = 2,\n",
    "                 markerstrokewidth = 0,\n",
    "                 label = \"$var (obs, fit)\"))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return (minimizer = out.minimizer, minimum = out.minimum, AD_fail = AD_fail)  \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean_square_rel_error (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note: The norm functions are a bit faster when the loops are written out explicitly, \n",
    "#compared with using broadcasting, hence why the loops are written out here. \n",
    "\n",
    "function mean_square_error(x,y; w = fill(1.0, length(x)))\n",
    "    sum = 0.0\n",
    "    for i=1:length(x)\n",
    "        sum += w[i]*(x[i] - y[i])^2\n",
    "    end\n",
    "    return sum/length(x)\n",
    "end\n",
    "\n",
    "function mean_abs_error(x,y; w = fill(1.0, length(x)))\n",
    "    sum = 0.0\n",
    "    for i=1:length(x)\n",
    "        sum += w[i]*abs(x[i] - y[i])\n",
    "    end\n",
    "    return sum/length(x)\n",
    "end\n",
    "\n",
    "function mean_abs_rel_error(x,y; w = fill(1.0, length(x)))\n",
    "    sum = 0.0\n",
    "    for i=1:length(x)\n",
    "        sum += w[i]*abs(x[i]/y[i] - 1)\n",
    "    end\n",
    "    return sum/length(x)\n",
    "end\n",
    "\n",
    "function mean_square_rel_error(x,y; w = fill(1.0, length(x)))\n",
    "    sum = 0.0\n",
    "    for i=1:length(x)\n",
    "        sum += w[i]*(x[i]/y[i] - 1)^2\n",
    "    end\n",
    "    return sum/length(x)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
